{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dataset1']\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "import os\n",
    "print(os.listdir(\"../input\"))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "training_path = '../input/dataset1/dataset1/train'\n",
    "test_path = '../input/dataset1/dataset1/test'\n",
    "validation_path = '../input/dataset1/dataset1/valid'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras import optimizers\n",
    "from keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 148, 148, 32)      896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 74, 74, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 72, 72, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 36, 36, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 34, 34, 128)       73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 17, 17, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 15, 15, 128)       147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 7, 7, 128)         0         \n",
      "=================================================================\n",
      "Total params: 240,832\n",
      "Trainable params: 240,832\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "classifier = Sequential()\n",
    "classifier.add(Conv2D(32, (3, 3), input_shape = (150, 150, 3), activation = 'relu'))\n",
    "classifier.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "classifier.add(Conv2D(64, (3, 3), activation = 'relu'))\n",
    "classifier.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "classifier.add(Conv2D(128, (3, 3), activation = 'relu'))\n",
    "classifier.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "classifier.add(Conv2D(128, (3, 3), activation = 'relu'))\n",
    "classifier.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "classifier.summary()\n",
    "classifier.add(Flatten())\n",
    "# classifier.add(layers.Dropout(0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "# Step 4 - Full connection\n",
    "classifier.add(Dense(units = 128, kernel_regularizer=regularizers.l1_l2(l1=0.001, l2=0.001), activation = 'relu'))\n",
    "classifier.add(Dropout(0.5))\n",
    "classifier.add(Dense(units = 64, kernel_regularizer=regularizers.l1_l2(l1=0.001, l2=0.001), activation = 'relu'))\n",
    "classifier.add(Dropout(0.5))\n",
    "# classifier.add(Dense(units = 64, activation = 'relu'))\n",
    "classifier.add(Dense(units = 2, activation = 'softmax'))\n",
    "#classifier.compile(optimizer = 'rmsprop', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "classifier.compile(optimizer = optimizers.RMSprop(lr=0.0001), loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "# classifier.compile(optimizer = optimizers.RMSprop(lr=1e-4), loss = 'categorical_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
    "                                   shear_range = 0.2,\n",
    "                                   zoom_range = 0.2,\n",
    "                                   horizontal_flip = True)\n",
    "valid_datagen = ImageDataGenerator(rescale = 1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1600 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "training_set = train_datagen.flow_from_directory(training_path,\n",
    "                                                 target_size = (150, 150),\n",
    "                                                 batch_size = 32,\n",
    "                                                 class_mode = 'categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 340 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "validation_set = train_datagen.flow_from_directory(validation_path,\n",
    "                                                 target_size = (150, 150),\n",
    "                                                 batch_size = 32,\n",
    "                                                 class_mode = 'categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/15\n",
      "100/100 [==============================] - 41s 410ms/step - loss: 10.0480 - acc: 0.5184 - val_loss: 6.9837 - val_acc: 0.5844\n",
      "Epoch 2/15\n",
      "100/100 [==============================] - 33s 333ms/step - loss: 4.8938 - acc: 0.5797 - val_loss: 3.1982 - val_acc: 0.5714\n",
      "Epoch 3/15\n",
      "100/100 [==============================] - 33s 331ms/step - loss: 2.3018 - acc: 0.6538 - val_loss: 1.7449 - val_acc: 0.7165\n",
      "Epoch 4/15\n",
      "100/100 [==============================] - 33s 326ms/step - loss: 1.5593 - acc: 0.7416 - val_loss: 1.4531 - val_acc: 0.7591\n",
      "Epoch 5/15\n",
      "100/100 [==============================] - 32s 323ms/step - loss: 1.3316 - acc: 0.8009 - val_loss: 1.2799 - val_acc: 0.8099\n",
      "Epoch 6/15\n",
      "100/100 [==============================] - 32s 322ms/step - loss: 1.1842 - acc: 0.8375 - val_loss: 1.1482 - val_acc: 0.8097\n",
      "Epoch 7/15\n",
      "100/100 [==============================] - 33s 333ms/step - loss: 1.0884 - acc: 0.8509 - val_loss: 1.0660 - val_acc: 0.8228\n",
      "Epoch 8/15\n",
      "100/100 [==============================] - 32s 320ms/step - loss: 1.0050 - acc: 0.8606 - val_loss: 0.9788 - val_acc: 0.8383\n",
      "Epoch 9/15\n",
      "100/100 [==============================] - 33s 328ms/step - loss: 0.9392 - acc: 0.8647 - val_loss: 0.9189 - val_acc: 0.8499\n",
      "Epoch 10/15\n",
      "100/100 [==============================] - 32s 318ms/step - loss: 0.8655 - acc: 0.8750 - val_loss: 0.8713 - val_acc: 0.8578\n",
      "Epoch 11/15\n",
      "100/100 [==============================] - 32s 323ms/step - loss: 0.8209 - acc: 0.8806 - val_loss: 0.8153 - val_acc: 0.8688\n",
      "Epoch 12/15\n",
      "100/100 [==============================] - 32s 322ms/step - loss: 0.7751 - acc: 0.8859 - val_loss: 0.7680 - val_acc: 0.8718\n",
      "Epoch 13/15\n",
      "100/100 [==============================] - 32s 321ms/step - loss: 0.7411 - acc: 0.8922 - val_loss: 0.7226 - val_acc: 0.8831\n",
      "Epoch 14/15\n",
      "100/100 [==============================] - 32s 320ms/step - loss: 0.6866 - acc: 0.8944 - val_loss: 0.7349 - val_acc: 0.8479\n",
      "Epoch 15/15\n",
      "100/100 [==============================] - 32s 317ms/step - loss: 0.6657 - acc: 0.8991 - val_loss: 0.6743 - val_acc: 0.8779\n"
     ]
    }
   ],
   "source": [
    "history = classifier.fit_generator(\n",
    "                        training_set,\n",
    "                        steps_per_epoch=100,\n",
    "                        epochs=15,\n",
    "                        validation_data=validation_set,\n",
    "                        validation_steps=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Epoch 1/2\n",
    "# 100/100 [==============================] - 149s 1s/step - loss: 0.6284 - acc: 0.6316 - val_loss: 0.5407 - val_acc: 0.7242\n",
    "# Epoch 2/2\n",
    "# 100/100 [==============================] - 149s 1s/step - loss: 0.4399 - acc: 0.8031 - val_loss: 0.3633 - val_acc: 0.8539"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.save('gender_classifier.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model = load_model('gender_classifier.h5')\n",
    "classifier = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_faces(gender, image_name):\n",
    "    #Making Single new prediction\n",
    "    import numpy as np\n",
    "    #from keras.preprocessing.image import image\n",
    "    import keras.preprocessing.image as image\n",
    "    test_image = image.load_img(f'../input/dataset1/dataset1/test/{gender}/{image_name}', target_size = (150, 150))\n",
    "    test_image = image.img_to_array(test_image)\n",
    "    #use this function to change the image from 2dimension(64, 64) to 3dimension(64, 64, 3)\n",
    "    test_image = np.expand_dims(test_image, axis=0)\n",
    "    #add one more dimension before predicting because the predict method expects a batch\n",
    "    result = classifier.predict(test_image)\n",
    "\n",
    "    #training_set.class_indices\n",
    "    prediction = ''\n",
    "    if result[0][0] == 1:\n",
    "        prediction = 'man'\n",
    "    elif result[0][1] == 1:\n",
    "        prediction = 'woman'\n",
    "    print(image_name, prediction)\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "face_7.jpg man\n",
      "face_39.jpg man\n",
      "face_154.jpg man\n",
      "face_153.jpg man\n",
      "face_178.jpg man\n",
      "face_152.jpg woman\n",
      "face_123.jpg man\n",
      "face_107.jpg man\n",
      "face_106.jpg man\n",
      "face_76.jpg woman\n",
      "face_25.jpg man\n",
      "face_54.jpg man\n",
      "face_124.jpg man\n",
      "face_33.jpg man\n",
      "face_168.jpg man\n",
      "face_171.jpg man\n",
      "face_21.jpg man\n",
      "face_45.jpg man\n",
      "face_105.jpg man\n",
      "face_19.jpg man\n",
      "face_138.jpg man\n",
      "face_63.jpg man\n",
      "face_65.jpg man\n",
      "face_158.jpg woman\n",
      "face_180.jpg man\n",
      "face_97.jpg man\n",
      "face_182.jpg man\n",
      "face_167.jpg man\n",
      "face_44.jpg man\n",
      "face_159.jpg man\n",
      "face_165.jpg \n",
      "face_50.jpg man\n",
      "face_139.jpg man\n",
      "face_175.jpg man\n",
      "face_71.jpg man\n",
      "face_10.jpg man\n",
      "face_11.jpg man\n",
      "face_96.jpg woman\n",
      "face_146.jpg woman\n",
      "face_144.jpg man\n",
      "face_82.jpg man\n",
      "face_133.jpg man\n",
      "face_90.jpg man\n",
      "face_49.jpg woman\n",
      "face_118.jpg man\n",
      "face_1.jpg man\n",
      "face_2.jpg man\n",
      "face_74.jpg man\n",
      "face_181.jpg man\n",
      "face_95.jpg man\n",
      "face_93.jpg man\n",
      "face_40.jpg man\n",
      "face_53.jpg man\n",
      "face_101.jpg man\n",
      "face_67.jpg man\n",
      "face_174.jpg man\n",
      "face_27.jpg man\n",
      "face_29.jpg man\n",
      "face_12.jpg man\n",
      "face_43.jpg man\n",
      "face_132.jpg man\n",
      "face_55.jpg woman\n",
      "face_61.jpg man\n",
      "face_103.jpg man\n",
      "face_77.jpg man\n",
      "face_41.jpg man\n",
      "face_4.jpg man\n",
      "face_169.jpg man\n",
      "face_156.jpg man\n",
      "face_164.jpg man\n",
      "face_176.jpg woman\n",
      "face_69.jpg man\n",
      "face_183.jpg woman\n",
      "face_136.jpg man\n",
      "face_151.jpg man\n",
      "face_48.jpg man\n",
      "face_34.jpg man\n",
      "face_127.jpg man\n",
      "face_83.jpg man\n",
      "face_26.jpg man\n",
      "face_135.jpg man\n",
      "face_117.jpg man\n",
      "face_98.jpg man\n",
      "face_32.jpg man\n",
      "face_20.jpg man\n",
      "face_60.jpg man\n",
      "face_92.jpg woman\n",
      "face_137.jpg woman\n",
      "face_88.jpg man\n",
      "face_78.jpg \n",
      "face_68.jpg man\n",
      "face_17.jpg man\n",
      "face_141.jpg man\n",
      "face_111.jpg man\n",
      "face_143.jpg man\n",
      "face_129.jpg man\n",
      "face_109.jpg woman\n",
      "face_157.jpg man\n",
      "face_31.jpg man\n",
      "face_149.jpg man\n",
      "face_134.jpg man\n",
      "face_125.jpg man\n",
      "face_6.jpg man\n",
      "face_108.jpg woman\n",
      "face_140.jpg man\n",
      "face_16.jpg man\n",
      "face_116.jpg man\n",
      "face_161.jpg man\n",
      "face_18.jpg man\n",
      "face_142.jpg man\n",
      "face_113.jpg man\n",
      "face_84.jpg man\n",
      "face_28.jpg man\n",
      "face_56.jpg man\n",
      "face_73.jpg man\n",
      "face_24.jpg man\n",
      "face_126.jpg man\n",
      "face_177.jpg man\n",
      "face_23.jpg man\n",
      "face_91.jpg man\n",
      "face_30.jpg woman\n",
      "face_58.jpg man\n",
      "face_79.jpg man\n",
      "face_35.jpg woman\n",
      "face_5.jpg man\n",
      "face_115.jpg man\n",
      "face_89.jpg man\n",
      "face_172.jpg man\n",
      "face_47.jpg woman\n",
      "face_22.jpg man\n",
      "face_128.jpg man\n",
      "face_13.jpg man\n",
      "face_0.jpg man\n",
      "face_42.jpg man\n",
      "face_104.jpg woman\n",
      "face_114.jpg man\n",
      "face_3.jpg man\n",
      "face_66.jpg man\n",
      "face_160.jpg man\n",
      "face_179.jpg man\n",
      "face_110.jpg man\n",
      "face_72.jpg man\n",
      "face_99.jpg man\n",
      "face_80.jpg man\n",
      "face_131.jpg man\n",
      "face_122.jpg man\n",
      "face_119.jpg man\n",
      "face_85.jpg man\n",
      "face_14.jpg man\n",
      "face_94.jpg woman\n",
      "face_170.jpg man\n",
      "face_87.jpg man\n",
      "face_8.jpg \n",
      "face_148.jpg man\n",
      "face_162.jpg man\n",
      "face_112.jpg man\n",
      "face_64.jpg woman\n",
      "face_173.jpg woman\n",
      "face_57.jpg man\n",
      "face_120.jpg \n",
      "face_51.jpg man\n",
      "face_75.jpg man\n",
      "face_38.jpg man\n",
      "face_36.jpg man\n",
      "face_130.jpg man\n",
      "face_163.jpg man\n",
      "face_52.jpg man\n",
      "face_147.jpg man\n",
      "face_9.jpg man\n",
      "face_145.jpg woman\n",
      "face_166.jpg woman\n",
      "face_7.jpg woman\n",
      "face_39.jpg woman\n",
      "face_154.jpg woman\n",
      "face_153.jpg woman\n",
      "face_189.jpg woman\n",
      "face_152.jpg woman\n",
      "face_123.jpg woman\n",
      "face_107.jpg man\n",
      "face_106.jpg woman\n",
      "face_76.jpg man\n",
      "face_25.jpg woman\n",
      "face_54.jpg woman\n",
      "face_81.jpg woman\n",
      "face_124.jpg woman\n",
      "face_121.jpg woman\n",
      "face_59.jpg woman\n",
      "face_33.jpg woman\n",
      "face_168.jpg woman\n",
      "face_171.jpg woman\n",
      "face_21.jpg woman\n",
      "face_45.jpg woman\n",
      "face_105.jpg woman\n",
      "face_19.jpg woman\n",
      "face_138.jpg woman\n",
      "face_63.jpg woman\n",
      "face_65.jpg woman\n",
      "face_158.jpg woman\n",
      "face_180.jpg woman\n",
      "face_97.jpg woman\n",
      "face_182.jpg man\n",
      "face_167.jpg woman\n",
      "face_44.jpg \n",
      "face_159.jpg woman\n",
      "face_165.jpg woman\n",
      "face_50.jpg woman\n",
      "face_175.jpg woman\n",
      "face_71.jpg woman\n",
      "face_10.jpg woman\n",
      "face_11.jpg woman\n",
      "face_96.jpg woman\n",
      "face_146.jpg woman\n",
      "face_144.jpg woman\n",
      "face_82.jpg woman\n",
      "face_133.jpg woman\n",
      "face_90.jpg woman\n",
      "face_49.jpg woman\n",
      "face_118.jpg woman\n",
      "face_155.jpg woman\n",
      "face_1.jpg woman\n",
      "face_2.jpg woman\n",
      "face_74.jpg woman\n",
      "face_181.jpg woman\n",
      "face_95.jpg woman\n",
      "face_93.jpg woman\n",
      "face_40.jpg woman\n",
      "face_101.jpg woman\n",
      "face_67.jpg woman\n",
      "face_27.jpg woman\n",
      "face_29.jpg woman\n",
      "face_185.jpg woman\n",
      "face_37.jpg woman\n",
      "face_12.jpg woman\n",
      "face_43.jpg woman\n",
      "face_132.jpg woman\n",
      "face_55.jpg woman\n",
      "face_61.jpg man\n",
      "face_103.jpg woman\n",
      "face_41.jpg woman\n",
      "face_4.jpg woman\n",
      "face_169.jpg woman\n",
      "face_156.jpg woman\n",
      "face_176.jpg woman\n",
      "face_69.jpg woman\n",
      "face_136.jpg woman\n",
      "face_151.jpg man\n",
      "face_48.jpg woman\n",
      "face_34.jpg woman\n",
      "face_127.jpg woman\n",
      "face_83.jpg woman\n",
      "face_135.jpg woman\n",
      "face_98.jpg man\n",
      "face_32.jpg \n",
      "face_20.jpg woman\n",
      "face_60.jpg woman\n",
      "face_92.jpg man\n",
      "face_88.jpg woman\n",
      "face_15.jpg woman\n",
      "face_78.jpg woman\n",
      "face_68.jpg woman\n",
      "face_17.jpg woman\n",
      "face_111.jpg woman\n",
      "face_143.jpg woman\n",
      "face_129.jpg woman\n",
      "face_109.jpg woman\n",
      "face_157.jpg woman\n",
      "face_31.jpg woman\n",
      "face_149.jpg woman\n",
      "face_134.jpg man\n",
      "face_125.jpg woman\n",
      "face_6.jpg woman\n",
      "face_108.jpg woman\n",
      "face_16.jpg woman\n",
      "face_62.jpg woman\n",
      "face_116.jpg woman\n",
      "face_161.jpg woman\n",
      "face_18.jpg woman\n",
      "face_142.jpg woman\n",
      "face_113.jpg woman\n",
      "face_186.jpg woman\n",
      "face_84.jpg woman\n",
      "face_28.jpg woman\n",
      "face_86.jpg woman\n",
      "face_56.jpg woman\n",
      "face_73.jpg man\n",
      "face_24.jpg woman\n",
      "face_126.jpg woman\n",
      "face_188.jpg woman\n",
      "face_23.jpg woman\n",
      "face_91.jpg woman\n",
      "face_30.jpg woman\n",
      "face_58.jpg woman\n",
      "face_79.jpg woman\n",
      "face_35.jpg woman\n",
      "face_5.jpg woman\n",
      "face_150.jpg woman\n",
      "face_187.jpg woman\n",
      "face_115.jpg woman\n",
      "face_47.jpg woman\n",
      "face_22.jpg woman\n",
      "face_128.jpg woman\n",
      "face_13.jpg woman\n",
      "face_0.jpg woman\n",
      "face_42.jpg woman\n",
      "face_104.jpg woman\n",
      "face_114.jpg woman\n",
      "face_3.jpg woman\n",
      "face_66.jpg woman\n",
      "face_160.jpg woman\n",
      "face_72.jpg woman\n",
      "face_99.jpg woman\n",
      "face_80.jpg woman\n",
      "face_100.jpg woman\n",
      "face_122.jpg woman\n",
      "face_119.jpg man\n",
      "face_85.jpg woman\n",
      "face_14.jpg woman\n",
      "face_94.jpg woman\n",
      "face_170.jpg woman\n",
      "face_87.jpg woman\n",
      "face_8.jpg woman\n",
      "face_70.jpg woman\n",
      "face_148.jpg man\n",
      "face_162.jpg man\n",
      "face_112.jpg woman\n",
      "face_64.jpg woman\n",
      "face_173.jpg woman\n",
      "face_57.jpg woman\n",
      "face_120.jpg woman\n",
      "face_51.jpg woman\n",
      "face_46.jpg woman\n",
      "face_102.jpg woman\n",
      "face_38.jpg woman\n",
      "face_36.jpg woman\n",
      "face_130.jpg woman\n",
      "face_163.jpg man\n",
      "face_52.jpg woman\n",
      "face_147.jpg woman\n",
      "face_9.jpg woman\n",
      "face_145.jpg woman\n"
     ]
    }
   ],
   "source": [
    "man_images = os.listdir(\"../input/dataset1/dataset1/test/man\")\n",
    "woman_images = os.listdir(\"../input/dataset1/dataset1/test/woman\")\n",
    "res_man = []\n",
    "res_woman = []\n",
    "for image_name in man_images:\n",
    "    res_man.append(predict_faces('man', image_name))\n",
    "for image_name in woman_images:\n",
    "    res_woman.append(predict_faces('woman', image_name))\n",
    "# res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Counter({'man': 145, 'woman': 21, '': 4}),\n",
       " Counter({'woman': 155, 'man': 13, '': 2}))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "results = (Counter(res_man), Counter(res_woman))\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# women\n",
    "# Counter({'woman': 128, 'man': 39, '': 3}), 75.3% accuracy\n",
    "\n",
    "# Counter({'woman': 145, 'man': 22, '': 3}), 85.3% accuracy\n",
    "# Counter({'man': 154, 'woman': 12, '': 4}), 90.60% accuracy\n",
    "\n",
    "# men"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
